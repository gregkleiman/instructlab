What’s new in IBM Storage Ceph 7.1  

IBM Storage Ceph is an enterprise-grade distributed, universal,  software defined  storage solution, proven at scale   

An IBM Storage Ceph cluster can be installed by running a single  command.   

IBM Storage Ceph features a dashboard UI  and APIs for lights out  data center operations. 

IBM Storage Ceph is Flexible, scale-out  architecture running  on clustered x86 industry-standard  hardware  

IBM Storage Ready Nodes are Validated hardware for  IBM Storage Ceph, ready  for immediate use, with  single source support and  published performance  data.   

IBM Storage Ceph provides A single, efficient, unified storage platform for object,  block, and file storage  

IBM Storage Ceph brings together the best of the  open source ecosystem  while providing enterprise grade assurance through secure  development,  robust validation and end to end support  by IBM. 

IBM Storage Ceph initial cluster setup can start with a minimum of 4  industry-standard x86- servers running Red Hat  Enterprise Linux and  easily scale out to business needs.  

Or simply choose IBM’s  complete ready to run  solution of IBM Storage Ready nodes for IBM Storage Ceph 

IBM Storage Ceph  software internally runs Linux containers removing any needs for  specific dependencies making it More flexible, faster and  easier to deploy and  maintain, compared to  conventional package based software deployment.  

IBM Storage Ceph CLI-based  administration with cephadm  

Cephadm is a utility that deploys and manages  IBM Storage Ceph   
Cephadm is tightly  integrated with both the  command-line interface  (CLI) and the IBM Storage Ceph dashboard through a  web user interface.  

Clients can manage IBM storage Ceph clusters from the Ceph Dashboard Web-UI, an intuitive user interface that allows for  easy, straightforward navigation that delivers a  simplified experience  for common administrative tasks  

Key administrative tasks include managing storage  capacity, configuring  services, access for file,  block and object, object  buckets, users, and S3  access keys.  

IBM Storage Ceph  application integration offers  File, Block and Object  APIs and protocols to IBM Storage Ceph and support a broad variety  of applications and  platforms.  

IBM Storage Ceph  features a set of  industry standard and  consistent APIs for data  and service consumer interactivity and provides consistent and  standardized APIs such  as S3 for Object or  POSIX for File.  

IBM Storage Ceph leads  in the on-premise object  storage market when it  comes to AWS S3 fidelity and compatibility.  

IBM Storage Ceph easily meets growing  demands As a distributed storage  system 

 IBM Storage  Ceph scales effortlessly,  to meet growing data  demands and business needs as New nodes or devices can be added to the  cluster without having disruptions or service downtime.  

IBM Storage Ceph scale-out architecture works in two  dimensions: capacity  and throughput.   

This straightforward  scalability feature enables organizations to  adapt to evolving storage needs seamlessly. 

IBM Storage Ceph includes scalable management capabilities to manage  ever growing amounts  of unstructured data.  

Alongside scalable  cluster capacity and  throughput, management of the cluster resources can  also scale accordingly.  

IBM watsonx.data provides clients with a data platform that  contains Open-source query engines, open-source data serialization  libraries, and open- source table formats including Presto,  Alluxio, Iceberg, Parquet, Apache Spark, Apache Orc, and Avro  

IBM watsonx.data comes bundled with entitlements for  IBM Storage Ceph, An open-source enterprise level storage solution, with full support and lifecycle services by IBM.  

Licensing for 768  Terabytes of raw IBM Storage Ceph capacity comes included with  watsonx.data 
 

IBM watsonx.data with IBM Storage Ceph includes the following: 

Security to Store tables with S3 data encryption  

Security Token Service support for Role based governance  

D3N caching for D3N query acceleration, datacenter Data Delivery Network  

Natively implemented in IBM Storage Ceph RGW  

Parquet support for S3 Select pushdown  

Bucket notification of bucket events or changes; can be used in context of serverless runtimes, feeds into  messaging systems or similar. 

IBM Storage Ready nodes for Ceph are IBM qualified and supported hardware for running IBM’s Storage Ceph cluster platform that Scale from a minimum of 4 nodes to hundreds of nodes  

IBM Storage Ready nodes Expansion is possible with N+1 scalability.  

Ceph Ready Nodes can have NVMe- or SATA drives; Different capacity choices are available; Industry leading servers in a 2U server chassis   

IBM Storage ready nodes are validated hardware for IBM Storage Ceph  

IBM Storage Ready Nodes offer an easy, flexible and cost-effective way to deploy IBM Storage Ceph  IBM Storage Ready Nodes include published performance data.  

RHEL and IBM Storage Ceph are installed on client premises.   

IBM is the Single point of contact for procurement, support, services and lifecycle care.  

Quickly identify health, performance and capacity of IBM Storage Ceph cluster–or server nodes that need attention via Call home feature. 

IBM Storage Ceph NVMe over TCP is Block storage for VMware; Initial target use case is providing bare metal storage support.  

IBM Storage Ceph provides NVMe over TCP Target and Initiator combination to access  storage from the Ceph RADOS subsystem.   

IBM Storage Ceph provides Block storage for consumption by VMware.  

New management layer via Ceph-NVMe over TCP daemon, coordinates configuration of  NVMe over TCP targets, across multiple cluster nodes.  

No kernel dependencies since SPDK-based code is included 

NVMe over TCP volumes are supported as VMFS datastore  

Connect ESXi hypervisor’s NVMe over TCP Initiator to Ceph NVMe over TCP Target Access IBM Storage Ceph RBD block volume using NVMe over TCP. Create a VMFS6 VMware datastore based on IBM Storage Ceph VMWare 7 update 3 and above are supported.  

IBM Storage Ceph Provides IBM Storage Ceph RBD block volume Accessible by NVMe/TCP; This can be used as a VMware VMFS datastore for VMware vSphere  deployments. 

IBM Storage Ceph vSphere remote plug-in Initially will be primarily used with NVMe over TCP gateway.  

At a later stage, the plugin could be expanded to other protocols, supported  by IBM Storage Ceph for vSphere.  

This Allows the IBM Storage Ceph system to be managed and administered  via VMware vCenter.  

IBM Storage Ceph service integration into vCenter Server is not required for this  functionality. 

IBM Storage Ceph Volume Management with VMware vCenter Enables IBM Storage Ceph data volume management through VMware  vCenter for vSphere administration  including all life cycle operations of the Ceph block volumes as well as Health  and Performance monitoring with support on VMware 7 update 3 and above but This feature is only technical preview and not appropriate for production  environments 

IBM Storage Ceph performance and Health monitoring with VMware  vCenter Enables IBM Storage Ceph performance and health monitoring through the vCenter for vSphere admins and Covers all life cycle operations of the Ceph block volumes   

as well as Health and Performance monitoring for VMware 7 update 3 and above but This feature is only technical preview and not appropriate for production  environments 

IBM Storage Ceph NVMe over TCP provides the following value to IT operations personnel:  

Ability to serve IBM Storage Ceph RBD block storage to VMware.  

Consolidate storage sourcing and serve resilient block storage to VMware and other non-Linux environments 

IBM Storage Ceph NFS access capabilities on top of CephFS provide IBM Storage Ceph CephFS volume file access through NFS For legacy applications and MS Windows clients Supporting NFS versions NFSv4.1 and NFSv3  with Management of NFS clusters through CLI and dashboard UI  

IBM Storage Ceph CephFS volume file access through NFS provides Resilience against storage node failure with automatic failover and Supports hundreds of NFS shares including Support for external load balancer tooling like HAproxy and F5 snd Integrates with Kerberos. 

IBM Storage Ceph NFS access provides the following value to IT operations personnel:  

Ability to serve IBM Storage Ceph file storage to non-Linux clients using NFS.  

Consolidate infrastructure storage sourcing and serve file storage access to non-Linux environments 

IBM Storage Ceph RGW New supported S3-compatible platforms Transition data to a remote cloud service as part of the lifecycle configuration  using storage classes to reduce cost and improve manageability.   

IBM Storage Ceph RGW Data Transition can be unidirectional, but data cannot be transitioned back from  the remote zone.   

Data transition can be directed to multiple cloud providers, such as Amazon AWS and Microsoft Azure.   

The Key difference with Azure is that a multi-cloud gateway (MCG) is required to  translate S3 protocol to Azure Blob.  

IBM Storage Ceph RGW Intel QAT acceleration for Object compression and encryption  

 

Intel QAT (QuickAssist Technology) provides extended accelerated checksums, encryption  and compression services.   

By offloading actual check summing, encryption and compression request(s) to hardware QuickAssist accelerators it is More efficient in terms of cost and power than general-purpose CPUs for these types of compute-intensive workloads.  

Offload of check summing, compression and Encryption to Intel Quick Assist Technology accelerators improves IBM Storage Ceph RGW performance.  

Intel QAT reduces a node’s CPU usage and improves the RGW performance, when  compression and/or encryption is enabled. 

IBM Storage Ceph Multi-site replication with bucket granularity is the Ability to replicate a selected bucket or  group of buckets to a different IBM Storage Ceph cluster; Similar to RGW multisite functionality but now with bucket granular  support  

IBM Storage Ceph Multi-site replication includes Bi-directional replication of selected buckets and Active-active replication between two sites, with bucket granularity 

IBM Storage Ceph Multi-site replication Clients can do the following: 

Configure sync policies per zone group or bucket,  

have the ability for  fine grained replication control,  

enable/disable synchronization per bucket 

Enable full zone replication and opt-out on specific bucket  replication  

Replicate from one source bucket to many destination buckets. Source and destination bucket names can be different.  

Configure different data flows between zones. Symmetrical or  directional 

IBM Storage Ceph multi-site replication provides the  following value to IT operations personnel:  

1. Bucket granularity enables our business to replicate  selected and business relevant data to other  locations.”  

2. Useful features for co-locations, branch offices, and edge  locations.  

  

IBM Storage Ceph NFS with RGW backend Provides an easy way to ingest existing enterprise data from Linux and  Windows clients into a Ceph object store by dropping files on an easy to  access NFS fileshare.  

IBM Storage Ceph NFS with RGW backend Provides clients like data scientists with an easy way to export results from  analytics jobs to share results with consumers that cannot access object  storage.  

IBM Storage Ceph NFS with RGW backend Enables regular users on other platforms to ingest or access result data on  an IBM Storage Ceph Object store.  

Object data can be made available to applications or platforms which do  not have capabilities for S3 object access natively. 

 

IBM Storage Ceph Object archive zone General availability where The archive zone receives all objects from the production zones.  

IBM Storage Ceph Object archive zone keeps every version for every object, providing the user with an object catalog that contains the full history of the object.  

IBM Storage Ceph Object archive zone provides immutable objects that cannot be deleted or modified from RGW endpoints.  

IBM Storage Ceph Object archive zone has the Ability to recover data from the archive zone and Enables for recovery of any version of any object that existed on production sites.  

In case of data loss, ransomware or disaster recovery, all valid versions of all  objects can be recovered easily.  

IBM Storage Ceph Object archive zone is Also suitable for compliance related use cases. 

IBM Storage Ceph Object archive zone with bucket granularity has The Goal is to reduce data storage in the archive zone.  

System administrators may decide to disable replication to the archive zone for certain non-critical types of object data buckets.  

This Allows clients to enable or disable replication to the archive zone on a per object bucket case.  

Distinctions can be made based on a single bucket granular level. 

IBM Storage Ceph object archive zone provides the following  value to IT operations personnel:  

1. Ceph object archive zone enables our business to secure  object data from external harm, intrusion or manipulation. 2. Protection against ransomware or virus-alike intrusions. 3. Data is easily recoverable on a per object bucket basis.  

  

Cephadm RGW deployment provides Full bootstrapping of the RGW service for single-site and  multi-site deployments  

The Cephadm utility deploys and manages a IBM Storage Ceph cluster and is ightly integrated with both the command-line interface (CLI) and the IBM Storage Ceph Dashboard web interface managing IBM Storage Ceph clusters from either environment.  

Cephadm capability to execute complete bootstrapping for IBM Storage Ceph RGW service for both Single- and Multi-site deployments and Instantly deploys RGW into a production-ready state for client usability. 

 

IBM Storage Ceph Dashboard UI adds NFS metrics and monitoring including the following NFS utilization metrics 

NFS services health status  

NFS exports/shares status  

NFS exports/shares rate-limits  

NFS services status for example:  

NFS services health 3 up and 1 down  

Exports/Shares status up/down  

NFS services setting and management for example:  

Exports/Shares rate-limits like set/list/edit/delete  

Alarms for services status and rate-limiting.  

NFS monitoring for example:  

Utilization capacity  

IO-usage average read&writes in MB/sec 

Connected clients IP addresses 

 

The IBM Storage Ceph Dashboard now allows storage admins to view and manage Ceph Object Gateway bucket policies.  

Storage administrators can now create, edit and delete Ceph Object Gateway bucket policies  from within the IBM Storage Ceph dashboard.   

A bucket policy is a resource-based policy that can be used to grant access permissions to  selected Ceph S3 buckets.  

Permissions attached to the bucket apply to all objects inside the selected bucket; policies are written in JSON format. 

Dashboard RGW UI includes the following options: 

Labeled performance counters per user/bucket  

A capability to retrieve information on the operations happening per RGW  

The functionality offers an ability to get RGW performance counters with a per user, or per bucket  granularity including the following:  

total number of operations.  

numbers of operations and different users, accessing a bucket.  

Example operation labels include: PUT, GET, STATS, LIST, DELETE 

 

The IBM Storage Ceph Dashboard now provides a Dashboard CephFS UI option for CephFS volume management  

Which includes the Ability to manage the entire lifecycle of CephFS volumes, subvolumes and snapshots via the IBM Storage Ceph Dashboard UI.  

Including the following features: 

Monitor health status, current and historical capacity utilization for CephFS volumes and  subvolumes.  

Manage filesystem snapshots via CephFS snapshot scheduler  

Creation, setting options, and deletion of filesystem volumes, subvolumes and user access  management.  

See whether a particular filesystem or volume is consumed by layered services such as NFS  Ganesha.  

Create, schedule, delete or list snapshots and remote mirroring management tasks. 

IBM Storage Ceph now provides Hardware monitoring which Enables IBM Storage Ceph to provide an inventory of the cluster server hardware, identify  hardware issues and perform specific actions.  

Example actions on hardware nodes include: 

Light the chassis and/or HDD LEDs  

Perform firmware upgrades  

Manage power on|off, reboot nodes   

Hardware status healthy/unhealthy  

Hardware type/model  

Firmware version and/or revision  

High-level hardware status insight is presented on the IBM Storage Ceph dashboard landing page.  
